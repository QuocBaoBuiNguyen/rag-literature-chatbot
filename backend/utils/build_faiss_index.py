import os
import xml.etree.ElementTree as ET
import pickle
import numpy as np
import faiss
import openai
import json
from sentence_transformers import SentenceTransformer

# MODEL_NAME = "BAAI/bge-large-zh"  # hoáº·c "bge-base-vi"
MODEL_NAME = "shibing624/text2vec-base-chinese"  # hoáº·c "bge-base-vi"
FAISS_INDEX_PATH = "data/faiss_index.bin"
DOCS_PKL_PATH = "data/doc_chunks.pkl"

# PhÃ¢n cÃ¢u dÃ i thÃ nh cÃ¡c cÃ¢u nhá» cÃ³ ngá»¯ nghÄ©a
def split_chinese_sentence_openrouter(text):
    """
    Sá»­ dá»¥ng Qwen qua OpenRouter (free) Ä‘á»ƒ tÃ¡ch cÃ¢u tiáº¿ng Trung
    """
    client = openai.OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_AI_KEY")  # ÄÄƒng kÃ½ táº¡i openrouter.ai
    )

    prompt = f"""
    è¯·å°†ä¸‹é¢è¿™æ®µä¸­æ–‡æ–‡æœ¬åˆ†è§£æˆå¤šä¸ªæœ‰å®Œæ•´è¯­ä¹‰çš„çŸ­å¥ã€‚è¦æ±‚ï¼š
    1. æ¯ä¸ªçŸ­å¥éƒ½è¦æœ‰å®Œæ•´çš„è¯­æ³•ç»“æ„å’Œè¯­ä¹‰
    2. ä¿æŒåŸæ–‡çš„æ‰€æœ‰ä¿¡æ¯ä¸ä¸¢å¤±
    3. ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªçŸ­å¥
    4. å¦‚æœæ–‡æœ¬æ²¡æœ‰æ˜ç¡®è¯­ä¹‰ï¼Œè¯·è¯´æ˜å¹¶å°è¯•åˆç†åˆ†æ®µ

    åŸæ–‡ï¼š{text}
    """

    try:
        response = client.chat.completions.create(
            model="qwen/qwen-2.5-coder-32b-instruct",  # Free model
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        return response.choices[0].message.content
    except Exception as e:
        print(f"OpenRouter API Error: {e}")
        return None
    
# Láº¥y overlap cÃ¡c Ä‘oáº¡n Ä‘á»ƒ bao quÃ¡t ngá»¯ nghÄ©a: vÃ­ dá»¥ chunk1chunk2chunk3, chunk2chunk3chunk4
def split_overlap_sematic_chunks(full_text):
    split_txt = full_text.split('\n')
    # Bá» pháº§n tá»­ Ä‘áº§u vÃ  cuá»‘i
    trimmed = split_txt[1:-1]

    # Loáº¡i bá» cÃ¡c pháº§n tá»­ rá»—ng
    result = [item for item in trimmed if item]
    result = [''.join(result[i:i+3]).replace(" ", "") for i in range(len(result) - 2)]
    return result

# Chia STC thÃ nh cÃ¡c chunk nhá» (2-4 cÃ¢u) vá»›i sliding window = 1
def split_into_chunks(sentences, chunk_size=3, stride=1):
    chunks = []
    for i in range(0, len(sentences) - chunk_size + 1, stride):
        chunk = " ".join(sentences[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

def parse_xml_to_chunks(xml_path):
    tree = ET.parse(xml_path)
    root = tree.getroot()
    chunks = []
    chunk_id = 0

    for page in root.findall(".//PAGE"):
        page_id = page.attrib.get("ID", "")
        stcs = [stc.text.strip() for stc in page.findall(".//STC") if stc.text]
        
        if not stcs:
            continue
        
        
        # Call API cá»§a QWEN Ä‘á»ƒ táº¡o sematic riÃªng
        full_text = "".join(stcs)
        sematic_chunk_list = split_chinese_sentence_openrouter(full_text)
        
        
        
        if sematic_chunk_list != None and len(sematic_chunk_list) > 3:
            # Chia thÃ nh cÃ¡c Ä‘oáº¡n nhá»
            overlap_chunks_list = split_overlap_sematic_chunks(sematic_chunk_list)
            chunk_texts = overlap_chunks_list
        else: chunk_texts = split_into_chunks(stcs, chunk_size=3, stride=1)
        
        for text in chunk_texts:
            chunks.append({
                "text": text,
                "metadata": {
                    "page_id": page_id,
                    "chunk_id": f"{page_id}_{chunk_id}"
                }
            })
            chunk_id += 1

    print(f"âœ… ÄÃ£ táº¡o tá»•ng cá»™ng {len(chunks)} chunk tá»« file XML.")
    return chunks

def build_and_save_faiss_index(docs, model_name):
    model = SentenceTransformer(model_name)
    texts = [doc["text"] for doc in docs]

    print(f"ğŸ“„ Äang nhÃºng {len(texts)} Ä‘oáº¡n vÄƒn báº£n...")
    embeddings = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)  # âœ… normalize
    vectors = np.array(embeddings).astype("float32")

    print(f"ğŸ“¦ Vector nhÃºng cÃ³ shape: {vectors.shape}")
    index = faiss.IndexFlatIP(vectors.shape[1])  # âœ… cosine similarity
    index.add(vectors)

    # Save FAISS index
    faiss.write_index(index, FAISS_INDEX_PATH)

    # Save docs for later retrieval
    with open(DOCS_PKL_PATH, "wb") as f:
        pickle.dump(docs, f)

    print(f"âœ… FAISS index saved to: {FAISS_INDEX_PATH}")
    print(f"ğŸ“¦ Saved {len(docs)} chunks with metadata to: {DOCS_PKL_PATH}")