from rag.embedding import embed_query
from rag.vector_store import load_faiss_index, search_similar_chunks
from rag.content_retriever import load_documents
from rag import globals as rag_globals
from llm.chatbot_llm import generate_answer  # giáº£ sá»­ báº¡n dÃ¹ng LLM local

# Load FAISS index vÃ  documents 1 láº§n khi import
# index, embeddings = load_faiss_index()
# documents = load_documents()

def ask_llm_with_rag(question: str) -> str:
    # 1. Embed cÃ¢u há»i
    query_vec = embed_query(question, rag_globals.embeddings)
    print(f"ğŸ” CÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c nhÃºng: {query_vec[:10]}...")  # In ra 10 giÃ¡ trá»‹ Ä‘áº§u tiÃªn cá»§a vector

    # 2. TÃ¬m Ä‘oáº¡n tÆ°Æ¡ng tá»±
    print(f"ğŸ” TÃ¬m kiáº¿m Ä‘oáº¡n tÆ°Æ¡ng tá»± trong FAISS index.... Index {rag_globals.index}, Documents {rag_globals.documents[:5]}")
    top_chunks = search_similar_chunks(rag_globals.index, query_vec, rag_globals.documents, k=3)
    print(f"ğŸ” TÃ¬m tháº¥y {len(top_chunks)} Ä‘oáº¡n tÆ°Æ¡ng tá»± cho cÃ¢u há»i...")

    # 3. Táº¡o prompt cho LLM
    context = "\n\n".join(top_chunks)
    prompt = f"""Dá»±a trÃªn cÃ¡c Ä‘oáº¡n sau:

            {context}

            CÃ¢u há»i: {question}
            Tráº£ lá»i ngáº¯n gá»n, báº±ng tiáº¿ng Viá»‡t dá»… hiá»ƒu."""

    print(f"ğŸ” Prompt cho LLM: {prompt[:500]}...")  # In ra 100 kÃ½ tá»± Ä‘áº§u tiÃªn cá»§a prompt
    # 4. Gá»i LLM local sinh cÃ¢u tráº£ lá»i
    return generate_answer(prompt)
